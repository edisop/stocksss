# streamlit_app.py
# Streamlit UI that talks to the deployed Modal app *via the Modal Python SDK* (no HTTP endpoints).
# Optionally, on startup it can auto-deploy the Modal app by writing the embedded source to /tmp
# and running `modal deploy`. This keeps everything fully cloud.
#
# How it works:
#   - On first run, it can deploy the Modal app (toggle via AUTO_DEPLOY env var or sidebar button).
#   - Then it uses `modal.Function.lookup(APP_NAME, fn)` to call:
#         - list_plans()
#         - get_plan_with_live(id=None|<plan_id>)
#   - The Modal app (GPU/CPU) does all heavy lifting and plan storage.
#
# Secrets you can set in Streamlit Cloud:
#   MODAL_TOKEN_ID, MODAL_TOKEN_SECRET   -> Modal API keys (or hardcode below)
#   MODAL_APP_NAME                       -> defaults to "inmemory-latest-infer"
#   AUTO_DEPLOY                          -> "1" to deploy the embedded modal app on start
#
# Hardcode tokens (optional; not recommended in public repos):
# MODAL_TOKEN_ID_HARDCODE = "YOUR_TOKEN_ID"
# MODAL_TOKEN_SECRET_HARDCODE = "YOUR_TOKEN_SECRET"

from __future__ import annotations
import os, subprocess, tempfile, textwrap
from pathlib import Path
from typing import Dict, Any, List, Optional

import streamlit as st
import pandas as pd

# ---- Optional: hardcode tokens here (fallback if env/secrets missing) ----
MODAL_TOKEN_ID_HARDCODE = os.getenv("MODAL_TOKEN_ID_HARDCODE", "")
MODAL_TOKEN_SECRET_HARDCODE = os.getenv("MODAL_TOKEN_SECRET_HARDCODE", "")

# ---- Modal SDK setup ----
import modal
# Prefer Streamlit secrets / env vars
_token_id = os.getenv("MODAL_TOKEN_ID") or MODAL_TOKEN_ID_HARDCODE
_token_secret = os.getenv("MODAL_TOKEN_SECRET") or MODAL_TOKEN_SECRET_HARDCODE
if _token_id and _token_secret:
    # Make sure the SDK sees the creds
    os.environ["MODAL_TOKEN_ID"] = _token_id
    os.environ["MODAL_TOKEN_SECRET"] = _token_secret

APP_NAME = os.getenv("MODAL_APP_NAME", "inmemory-latest-infer")

# ---- Embedded Modal app source (exact copy) ----
MODAL_APP_SOURCE = "# modal_infer_inmemory_latest.py\n# Modalized \"latest-day\" inference + plan storage + web endpoints for Streamlit\n# - CPU function builds texts in memory (no files)\n# - GPU function loads your ckpt from a Modal Volume and scores items\n# - \"Plan\" = top-k allocation with buy prices & quantities recorded at creation\n# - Daily cron at 9am Australia/Melbourne creates a new plan\n# - Web endpoints:\n#     GET /list_plans              -> list available plans (oldest -> latest)\n#     GET /get_plan_with_live      -> latest plan with live prices & P&L\n#     GET /get_plan_with_live?id=PLAN_ID -> chosen plan with live prices & P&L\n#\n# Volumes expected:\n#   - model-cache (v2): contains your checkpoint at /ckpts/...\n#   - code-cache  (v1): contains /pkgs/interfusion_encoder-3.1 or .whl\n#   - plans-cache (v1): this script will create JSON files in /vol/plans\n#\n# Deploy:\n#   modal deploy modal_infer_inmemory_latest.py\n#\n# Local smoke test:\n#   modal run modal_infer_inmemory_latest.py\n#   (then curl the printed web URLs)\nfrom __future__ import annotations\n\nimport os, re, math, time, json, warnings\nfrom typing import List, Dict, Tuple, Optional, Any\nfrom dataclasses import dataclass\n\nimport modal\n\n# ============================================================\n#                === CHANGE-ME DEFAULTS (edit here) ===\n# ============================================================\nDEFAULT_CKPT: str = \"/vol/models/ckpts/biex_listmle_final.pt\"\nDEFAULT_INVEST_AMT: float = 1000.0\nDEFAULT_TOP_K: int = 20\nDEFAULT_TEMP: float = 2.0\nDEFAULT_MAX_CANDIDATES: int = 500\nDEFAULT_OVERRIDE_DATE: str = \"\"   # \"\" -> auto rollover with price coverage\n\n# ============================================================\n#                 Finnhub keys (hard-coded)\n# ============================================================\nDEFAULT_FINNHUB_KEYS = [\n    \"d1v4p9pr01qo0ln2h99gd1v4p9pr01qo0ln2h9a0\",\n    \"d2bg239r01qrj4ilm3l0d2bg239r01qrj4ilm3lg\",\n    \"d2bg2e9r01qrj4ilm6agd2bg2e9r01qrj4ilm6b0\",\n    \"d2bg2nhr01qrj4ilm8igd2bg2nhr01qrj4ilm8j0\",\n    \"d2bg3f1r01qrj4ilme4gd2bg3f1r01qrj4ilme50\",\n    \"d2bg3m9r01qrj4ilmfu0d2bg3m9r01qrj4ilmfug\",\n    \"d2bg3upr01qrj4ilmhqgd2bg3upr01qrj4ilmhr0\",\n    \"d2bg451r01qrj4ilmja0d2bg451r01qrj4ilmjag\",\n    \"d2bg4vhr01qrj4ilmmp0d2bg4vhr01qrj4ilmmpg\",\n    \"d2bg5c9r01qrj4ilmpb0d2bg5c9r01qrj4ilmpbg\",\n]\n\n# -----------------------------\n# Modal app, images, volumes\n# -----------------------------\napp = modal.App(\"inmemory-latest-infer\")\n\nbase_image = (\n    modal.Image.debian_slim(python_version=\"3.11\")\n    .pip_install(\n        # build helpers\n        \"pip>=25.0\", \"setuptools>=70\", \"wheel>=0.43\",\n        # data + utils\n        \"pandas>=2.2\", \"numpy>=1.26\", \"tqdm>=4.66\",\n        # finance / APIs\n        \"yfinance>=0.2.50\", \"finnhub-python>=2.4.18\",\n        # models\n        \"transformers>=4.46\", \"tokenizers>=0.20\",\n    )\n)\n\ngpu_image = base_image.pip_install(\"torch==2.5.*\")\n\nMODEL_VOL = modal.Volume.from_name(\"model-cache\", create_if_missing=True, version=2)\nCODE_VOL  = modal.Volume.from_name(\"code-cache\",  create_if_missing=False, version=1)\nPLANS_VOL = modal.Volume.from_name(\"plans-cache\", create_if_missing=True, version=1)\n\n# -----------------------------\n# Static config\n# -----------------------------\nTICKERS_UNIVERSE = [\n    'APTV','KEYS','LII','YUM','BX','MSCI','NKE','WELL','NVR','RJF','BF-B','GWW','NCLH','JCI','URI','SPGI','BLDR','INTU',\n    'BXP','DOV','FTNT','MDT','LVS','STE','TROW','MAS','BG','ALB','CRM','HAL','PKG','VRTX','HRL','DLR','DAL','MLM','AON',\n    'GE','NWS','AMZN','LUV','C','SBUX','AIG','SHW','KEY','AME','EBAY','DECK','MRK','L','EQIX','NTAP','COIN','VICI','LH',\n    'EQT','PTC','KHC','KMB','IVZ','ABT','CINF','CMCSA','TRMB','RSG','EA','CNC','PFE','TPL','GPC','DIS','CTSH','O','CAT',\n    'PPG','AKAM','RVTY','BR','OMC','ORLY','CFG','NVDA','WBD','FFIV','BEN','SYF','WDAY','CHTR','RCL','MNST','PYPL','PEP',\n    'V','FDX','LW','GRMN','CPT','APH','LLY','UHS','WDC','DD','TMO','MTD','TDY','HIG','XOM','HLT','JBL','D','JBHT','CLX',\n    'K','IRM','COO','OTIS','EOG','ES','HPQ','EMN','TYL','UPS','PSX','LMT','AVGO','DVN','CVS','FE','ECL','J','STT','AXON',\n    'ELV','HOLX','GIS','MTB','AVB','NDAQ','CRWD','LYB','BA','DRI','MAR','MHK','GEHC','MGM','XEL','WY','AIZ','GILD','MET',\n    'DUK','TKO','LYV','NDSN','MSFT','EXC','EW','VLTO','FSLR','BALL','CPB','GEV','CDW','EXPE','ITW','TXN','MA','WEC','ROST',\n    'TRGP','PCG','EMR','HWM','ON','SPG','STLD','ENPH','TTD','MTCH','NEM','CPRT','HUBB','ANET','EPAM','ALLE','WTW','AMCR',\n    'JKHY','CTVA','IR','CSCO','DGX','TFC','COF','VTRS','AAPL','NTRS','CAG','NEE','EL','CPAY','EXPD','ALGN','DLTR','IDXX',\n    'LHX','RL','CF','CSGP','TXT','POOL','OXY','ADSK','HPE','PM','BRK-B','GPN','HON','NWSA','CNP','TDG','ADBE','IP','MMM',\n    'APO','SNPS','RMD','KMX','GDDY','KMI','FDS','NFLX','VST','ABBV','EXE','MMC','BSX','OKE','PNW','KLAC','DE','DELL',\n    'MOH','STX','FOX','VMC','MDLZ','CMI','SLB','MS','MCK','HAS','CMS','PCAR','FITB','WMB','GEN','WAT','CSX','WYNN','META',\n    'DG','NXPI','SW','VRSN','EVRG','KVUE','MO','PGR','AMP','SYY','WRB','UDR','PAYX','BMY','HSY','ETN','PODD','DHR','PH',\n    'MKC','INVH','TECH','HST','GOOGL','NUE','F','HII','APA','FAST','BDX','BIIB','LIN','MPC','HUM','IFF','SWKS','TSLA',\n    'NRG','PNC','BBY','CTAS','ACN','T','CMG','MOS','AES','BAX','WFC','AVY','DTE','GOOG','CEG','PHM','IPG','LEN','TER',\n    'WSM','AFL','MAA','SMCI','CCI','FOXA','GL','VTR','GNRC','BKNG','PEG','FI','PNR','SJM','UNH','MCHP','ADP','MCO','UNP',\n    'ROP','AXP','CI','INTC','XYZ','EIX','PAYC','FIS','ISRG','ATO','DXCM','DOW','FICO','CHD','DPZ','KIM','PLD','ROL','KKR',\n    'WST','DOC','KR','TRV','INCY','KDP','TGT','PSA','TJX','GM','EQR','MKTX','AMD','USB','AZO','DAY','BLK','AMAT','DVA',\n    'EG','JPM','REGN','ADM','PPL','STZ','SRE','NOW','BAC','MU','TT','APD','CRL','VZ','AWK','ESS','VRSK','SBAC','ETR','FRT',\n    'GD','MPWR','ORCL','HSIC','CDNS','PANW','FTV','LKQ','TSN','AMGN','PFG','ERIE','SOLV','WAB','BRO','TMUS','SWK','WMT',\n    'RF','CTRA','KO','HCA','ARE','SNA','TAP','COST','GLW','LDOS','HD','MCD','DHI','FCX','NI','COP','LNT','REG','ADI','MRNA',\n    'COR','CL','IBM','CZR','CCL','IQV','XYL','MSI','SYK','TPR','A','CBOE','JNJ','CAH','CARR','CME','ED','TSCO','TEL','BK',\n    'WM','CVX','PRU','CBRE','CB','UAL','AEE','ROK','UBER','ODFL','CHRW','IEX','WBA','FANG','AOS','AMT','ZBRA','GS','ULTA',\n    'PWR','DASH','ALL','LULU','IT','BKR','SO','RTX','LRCX','AJG','ICE','PG','QCOM','DDOG','ACGL','EFX','ABNB','NOC','NSC',\n    'AEP','SCHW','EXR','PLTR','HBAN','TTWO','VLO','LOW'\n]\nNEWS_1D_HOURS = int(os.getenv(\"NEWS_1D_HOURS\", \"48\"))\nNEWS_7D_DAYS  = 7\nRECENCY_HALFLIFE_DAYS = float(os.getenv(\"RECENCY_HALFLIFE_DAYS\", \"2.0\"))\nTICKER_MENTION_BONUS  = float(os.getenv(\"TICKER_MENTION_BONUS\", \"0.15\"))\nPER_SOURCE_LIMIT      = int(os.getenv(\"PER_SOURCE_LIMIT\", \"50\"))\nUTC_ROLLOVER_HOUR     = int(os.getenv(\"UTC_ROLLOVER_HOUR\", \"16\"))\nPRICE_LOOKBACK_DAYS   = 120\nSLEEP_BETWEEN_NEWS_TICKERS = float(os.getenv(\"SLEEP_BETWEEN_NEWS_TICKERS\", \"0.10\"))\n\n# ============================================================\n#               Helpers\n# ============================================================\nimport pandas as pd\nimport numpy as np\n\n_CLEAN_TAIL_RE   = re.compile(r\"\\s*(\u2014|-)\\s*[A-Z][A-Za-z&.\\s]+$\")\n_PARENS_TAIL_RE  = re.compile(r\"\\s*\\([^()]*\\)\\s*$\")\n_WS_RE           = re.compile(r\"\\s+\")\n\ndef _clean_headline(h: str) -> str:\n    if not isinstance(h, str): return \"\"\n    s = h.strip()\n    s = _CLEAN_TAIL_RE.sub(\"\", s)\n    s = _PARENS_TAIL_RE.sub(\"\", s)\n    s = s.strip(\" .:;,-\")\n    s = _WS_RE.sub(\" \", s)\n    return s\n\ndef _norm_key(h: str) -> str:\n    s = _clean_headline(h).lower()\n    s = re.sub(r\"[^a-z0-9\\s]\", \"\", s)\n    return _WS_RE.sub(\" \", s).strip()\n\ndef _normalize_news_datetime(df: pd.DataFrame, col: str = \"datetime\") -> pd.DataFrame:\n    if df is None or df.empty or col not in df.columns: return df if df is not None else pd.DataFrame()\n    df = df.copy()\n    def _p(v):\n        try:\n            if isinstance(v, (int, np.integer)) or (isinstance(v, str) and v.isdigit()):\n                return pd.to_datetime(int(v), unit=\"s\", utc=True)\n            return pd.to_datetime(v, utc=True, errors=\"coerce\")\n        except Exception:\n            return pd.NaT\n    df[col] = [_p(v) for v in df[col]]\n    df[col] = pd.to_datetime(df[col], utc=True, errors=\"coerce\").dt.tz_convert(\"UTC\").dt.tz_localize(None)\n    return df.dropna(subset=[col]).sort_values(col)\n\ndef _dedup_news(df: pd.DataFrame) -> pd.DataFrame:\n    if df is None or df.empty: return df if df is not None else pd.DataFrame()\n    keys = [k for k in (\"id\",\"headline\",\"datetime\",\"source\") if k in df.columns]\n    if \"id\" in keys:\n        return df.drop_duplicates(subset=[\"id\"])\n    if {\"headline\",\"datetime\"}.issubset(df.columns):\n        return df.drop_duplicates(subset=[\"headline\",\"datetime\"])\n    return df.drop_duplicates()\n\ndef _pct(x0: float, x1: float) -> Optional[float]:\n    try: return (x1 / x0 - 1.0) * 100.0\n    except Exception: return None\n\ndef _recency_score(dt: pd.Timestamp, ref: pd.Timestamp) -> float:\n    days = max(0.0, (ref - dt).total_seconds()) / 86400.0\n    lam = math.log(2.0) / max(1e-6, RECENCY_HALFLIFE_DAYS)\n    return math.exp(-lam * days)\n\ndef _select_diverse_by_source(df: pd.DataFrame, k: int, ref_dt: pd.Timestamp, ticker: Optional[str] = None) -> pd.DataFrame:\n    if df is None or df.empty or k <= 0:\n        return df.iloc[0:0] if isinstance(df, pd.DataFrame) else pd.DataFrame()\n    x = df.copy()\n    x[\"recency\"] = x[\"datetime\"].map(lambda d: _recency_score(pd.to_datetime(d), ref_dt))\n    x[\"mention_bonus\"] = 0.0 if not ticker else x[\"headline\"].str.contains(re.escape(ticker), case=False, na=False).astype(float) * TICKER_MENTION_BONUS\n    x[\"score\"] = x[\"recency\"] + x[\"mention_bonus\"]\n    x = x.sort_values([\"score\",\"datetime\"], ascending=[False, False])\n\n    src = \"source\" if \"source\" in x.columns else None\n    if not src:\n        return x.head(k)\n\n    buckets: Dict[str, List[pd.Series]] = {}\n    for _, row in x.iterrows():\n        sname = row[src] if isinstance(row[src], str) and row[src] else \"?\"\n        buckets.setdefault(sname, []).append(row)\n\n    cap = max(1, min(PER_SOURCE_LIMIT, int(math.ceil(k / max(1, len(buckets))))))\n    out = []\n    for rows in buckets.values():\n        rows_sorted = sorted(rows, key=lambda r: (r[\"score\"], r[\"datetime\"]), reverse=True)\n        out.extend(rows_sorted[:cap])\n    if len(out) < k:\n        used = set(map(id, out))\n        remaining = []\n        for rows in buckets.values():\n            for r in rows:\n                if id(r) not in used:\n                    remaining.append(r)\n        remaining.sort(key=lambda r: (r[\"score\"], r[\"datetime\"]), reverse=True)\n        out.extend(remaining[:k-len(out)])\n    sel = pd.DataFrame(out).drop_duplicates(subset=[\"norm\"]) if out else x.iloc[0:0]\n    return sel.head(k)\n\ndef _window_time(df: pd.DataFrame, end_date: str, hours_1d: int, days_7d: int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    if df is None or df.empty:\n        return df.iloc[0:0], df.iloc[0:0]\n    end_dt = pd.to_datetime(end_date).normalize() + pd.Timedelta(days=1)\n    start_1d = end_dt - pd.Timedelta(hours=hours_1d)\n    start_7d = end_dt - pd.Timedelta(days=days_7d)\n    w1 = df.loc[(df[\"datetime\"] >= start_1d) & (df[\"datetime\"] < end_dt)].copy()\n    w7 = df.loc[(df[\"datetime\"] >= start_7d) & (df[\"datetime\"] < end_dt)].copy()\n    if w1.empty and w7.empty:\n        return w1, w7\n    w1 = w1.drop_duplicates(subset=[\"norm\"]).sort_values(\"datetime\")\n    n1 = set(w1[\"norm\"].tolist())\n    w7_excl = w7.loc[~w7[\"norm\"].isin(n1)].drop_duplicates(subset=[\"norm\"]).sort_values(\"datetime\")\n    return w1, w7_excl\n\ndef _market_macro_block(for_date: str, px: Dict[str, pd.DataFrame]) -> str:\n    out = \"--- Macro Indicators ---\\n\"\n    d = pd.to_datetime(for_date)\n    g = px.get(\"^GSPC\"); v = px.get(\"^VIX\")\n    if g is not None and not g.empty:\n        s = g.loc[g.index <= d]\n        if s.shape[0] >= 2:\n            pct1 = _pct(float(s[\"Close\"].iloc[-2]), float(s[\"Close\"].iloc[-1]))\n            if pct1 is not None: out += f\"S&P 500 (1D Change): {pct1:.2f}%\\n\"\n        if s.shape[0] >= 7:\n            s7 = s.iloc[-7:]\n            pct7 = _pct(float(s7[\"Close\"].iloc[0]), float(s7[\"Close\"].iloc[-1]))\n            if pct7 is not None: out += f\"S&P 500 (7D Change): {pct7:.2f}%\\n\"\n    if v is not None and not v.empty:\n        s2 = v.loc[v.index <= d]\n        if not s2.empty:\n            out += f\"VIX Level (as of {s2.index[-1].date()}): {float(s2['Close'].iloc[-1]):.2f}\\n\"\n    return out\n\ndef _stock_indicators(ticker: str, for_date: str, px: Dict[str, pd.DataFrame]) -> str:\n    hdr = f\"--- Ticker Indicators ---\\n\"\n    p = px.get(ticker); g = px.get(\"^GSPC\")\n    d = pd.to_datetime(for_date)\n    if p is None or p.empty:\n        return hdr + \"(no price coverage)\\n\"\n    s = p.loc[p.index <= d]\n    if s.shape[0] >= 1:\n        last_close = float(s[\"Close\"].iloc[-1])\n        hdr += f\"Last Close: {last_close:.2f}\\n\"\n    if s.shape[0] >= 2:\n        d1 = _pct(float(s[\"Close\"].iloc[-2]), float(s[\"Close\"].iloc[-1]))\n        if d1 is not None: hdr += f\"1D Change: {d1:.2f}%\\n\"\n    if s.shape[0] >= 7:\n        s7 = s.iloc[-7:]\n        d7 = _pct(float(s7[\"Close\"].iloc[0]), float(s7[\"Close\"].iloc[-1]))\n        if d7 is not None: hdr += f\"7D Change: {d7:.2f}%\\n\"\n    if s.shape[0] >= 20:\n        s20 = s.iloc[-20:]\n        d20 = _pct(float(s20[\"Close\"].iloc[0]), float(s20[\"Close\"].iloc[-1]))\n        if d20 is not None: hdr += f\"20D Change: {d20:.2f}%\\n\"\n        rets = np.diff(np.log(s20[\"Close\"].values))\n        if rets.size >= 2:\n            vol20 = float(np.std(rets, ddof=1) * math.sqrt(252) * 100.0)\n            hdr += f\"20D Vol (ann.): {vol20:.2f}%\\n\"\n    if g is not None and not g.empty and s.shape[0] >= 61 and g.shape[0] >= 61:\n        rt = np.diff(np.log(s[\"Close\"].iloc[-61:].values))\n        rs = np.diff(np.log(g[\"Close\"].iloc[-61:].values))\n        if rt.size == rs.size and rt.size >= 10 and np.var(rs) > 0:\n            beta = float(np.cov(rt, rs)[0,1] / np.var(rs))\n            hdr += f\"Beta (60D vs SPX): {beta:.2f}\\n\"\n    return hdr\n\ndef _format_lines(headlines: List[str]) -> str:\n    lines = [f\"- {str(h).strip()}\" for h in headlines if str(h).strip()]\n    return \"\\n\".join(lines) if lines else \"None\"\n\ndef _flatten_prices_multi(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n    out: Dict[str, pd.DataFrame] = {}\n    if df is None or df.empty: return out\n    if isinstance(df.columns, pd.MultiIndex):\n        for ticker in sorted(set(lvl for lvl in df.columns.get_level_values(0) if isinstance(lvl, str))):\n            sub = df[ticker].copy()\n            sub.index = pd.to_datetime(sub.index)\n            if \"Close\" not in sub.columns:\n                if \"Adj Close\" in sub.columns:\n                    sub = sub.rename(columns={\"Adj Close\": \"Close\"})\n                else:\n                    num_cols = [c for c in sub.columns if np.issubdtype(sub[c].dtype, np.number)]\n                    if num_cols:\n                        sub = sub.rename(columns={num_cols[-1]: \"Close\"})\n            out[ticker] = sub.sort_index()\n    else:\n        dd = df.copy()\n        dd.index = pd.to_datetime(dd.index)\n        if \"Close\" not in dd.columns and \"Adj Close\" in dd.columns:\n            dd = dd.rename(columns={\"Adj Close\": \"Close\"})\n        out[\"SINGLE\"] = dd.sort_index()\n    return out\n\ndef _effective_utc_date_for_infer() -> str:\n    now_utc = pd.Timestamp.utcnow()\n    use = now_utc.date() if now_utc.hour >= UTC_ROLLOVER_HOUR else (now_utc - pd.Timedelta(days=1)).date()\n    return pd.Timestamp(use).strftime(\"%Y-%m-%d\")\n\n# ---------- Plan storage helpers (on /vol/plans) ----------\ndef _plans_dir() -> str:\n    p = \"/vol/plans\"\n    os.makedirs(p, exist_ok=True)\n    return p\n\ndef _plan_path(plan_id: str) -> str:\n    return os.path.join(_plans_dir(), f\"{plan_id}.json\")\n\ndef _list_plan_files() -> List[str]:\n    d = _plans_dir()\n    try:\n        return sorted([f for f in os.listdir(d) if f.endswith(\".json\")])\n    except FileNotFoundError:\n        return []\n\ndef _load_plan(plan_id: str) -> Optional[Dict[str, Any]]:\n    path = _plan_path(plan_id)\n    if not os.path.exists(path):\n        return None\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        return json.load(f)\n\ndef _save_plan(plan: Dict[str, Any]) -> None:\n    path = _plan_path(plan[\"plan_id\"])\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(plan, f, separators=(\",\", \":\"), ensure_ascii=False)\n\ndef _list_plans_meta() -> List[Dict[str, Any]]:\n    out = []\n    for fn in _list_plan_files():\n        try:\n            with open(os.path.join(_plans_dir(), fn), \"r\", encoding=\"utf-8\") as f:\n                p = json.load(f)\n            out.append({\n                \"plan_id\": p.get(\"plan_id\"),\n                \"date\": p.get(\"date\"),\n                \"created_at\": p.get(\"created_at\"),\n                \"label\": f\"{p.get('date')} (created {p.get('created_at')})\",\n            })\n        except Exception:\n            continue\n    out = [x for x in out if x.get(\"plan_id\")]\n    out.sort(key=lambda r: r.get(\"created_at\") or \"\")\n    return out\n\ndef _latest_plan_id() -> Optional[str]:\n    metas = _list_plans_meta()\n    if not metas:\n        return None\n    return metas[-1][\"plan_id\"]\n\n# ============================================================\n#                Remote: CPU fetcher (build texts)\n# ============================================================\n@app.function(image=base_image, timeout=60*20)\ndef build_payload_remote(\n    for_date: str | None,\n    tickers: List[str],\n    max_candidates: int = 500,\n) -> Dict[str, Any]:\n    import yfinance as yf\n    import finnhub\n    from tqdm import tqdm as _tqdm\n\n    date_eff = (for_date or \"\").strip() or _effective_utc_date_for_infer()\n\n    start = (pd.to_datetime(date_eff) - pd.Timedelta(days=PRICE_LOOKBACK_DAYS + 5)).strftime(\"%Y-%m-%d\")\n    end   = (pd.to_datetime(date_eff) + pd.Timedelta(days=1)).strftime(\"%Y-%m-%d\")\n    all_syms = list(dict.fromkeys(tickers + [\"^GSPC\", \"^VIX\"]))\n    data = yf.download(all_syms, start=start, end=end, auto_adjust=True, progress=False, group_by=\"ticker\", threads=True)\n    px = _flatten_prices_multi(data)\n\n    g = px.get(\"^GSPC\")\n    if g is not None and not g.empty:\n        last_px_date = pd.to_datetime(g.index.max()).strftime(\"%Y-%m-%d\")\n        if pd.to_datetime(date_eff) > pd.to_datetime(last_px_date):\n            date_eff = last_px_date\n\n    key_list = list(DEFAULT_FINNHUB_KEYS)\n    if not key_list:\n        raise RuntimeError(\"DEFAULT_FINNHUB_KEYS is empty.\")\n    clients = [finnhub.Client(api_key=k) for k in key_list]\n    idx = 0\n\n    def _company_news(sym: str, start_date: str, end_date: str):\n        nonlocal idx\n        cli = clients[idx]\n        idx = (idx + 1) % len(clients)\n        try:\n            return cli.company_news(sym, _from=start_date, to=end_date) or []\n        except Exception:\n            time.sleep(0.6)\n            try:\n                return clients[idx].company_news(sym, _from=start_date, to=end_date) or []\n            except Exception:\n                return []\n\n    start_news = (pd.to_datetime(date_eff) - pd.Timedelta(days=NEWS_7D_DAYS)).strftime(\"%Y-%m-%d\")\n\n    news_by_ticker: Dict[str, pd.DataFrame] = {}\n    for t in _tqdm(tickers, desc=\"company_news\"):\n        raw = _company_news(t, start_news, date_eff)\n        rows = []\n        for it in raw:\n            rows.append({\n                \"id\": it.get(\"id\"),\n                \"datetime\": it.get(\"datetime\"),\n                \"headline\": (it.get(\"headline\") or \"\").strip(),\n                \"source\": (it.get(\"source\") or \"\").strip(),\n                \"ticker\": t,\n            })\n        df = pd.DataFrame(rows)\n        if not df.empty:\n            df = _normalize_news_datetime(df, \"datetime\")\n            df[\"headline\"] = df[\"headline\"].astype(str).map(_clean_headline)\n            df[\"norm\"] = df[\"headline\"].map(_norm_key)\n            df = _dedup_news(df).sort_values(\"datetime\").reset_index(drop=True)\n        else:\n            df = pd.DataFrame(columns=[\"datetime\",\"headline\",\"norm\",\"source\",\"ticker\"])\n        news_by_ticker[t] = df\n        time.sleep(SLEEP_BETWEEN_NEWS_TICKERS)\n\n    frames = []\n    for t in tickers:\n        df = news_by_ticker.get(t)\n        if df is not None and not df.empty:\n            frames.append(df[[\"datetime\",\"headline\",\"norm\",\"source\"]])\n    mkt_agg = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(columns=[\"datetime\",\"headline\",\"norm\",\"source\"])\n\n    def _market_text(for_date: str, mkt_agg: pd.DataFrame, px: Dict[str, pd.DataFrame], k1: int = 100, k7: int = 100):\n        hdr = f\"CTX:MARKET | DATE:{for_date}\\n\" + _market_macro_block(for_date, px)\n        if mkt_agg is None or mkt_agg.empty:\n            return hdr + \"NEWS_1D:\\nNone\\nNEWS_7D_EXCLUDING_1D:\\nNone\", 0, 0\n        w1, w7_ex = _window_time(mkt_agg, for_date, NEWS_1D_HOURS, NEWS_7D_DAYS)\n        ref_dt = pd.to_datetime(for_date) + pd.Timedelta(hours=23, minutes=59)\n        m1 = _select_diverse_by_source(w1, k=k1, ref_dt=ref_dt)\n        m7 = _select_diverse_by_source(w7_ex, k=k7, ref_dt=ref_dt)\n        hdr += \"NEWS_1D:\\n\" + _format_lines(m1[\"headline\"].tolist())\n        hdr += \"\\nNEWS_7D_EXCLUDING_1D:\\n\" + _format_lines(m7[\"headline\"].tolist())\n        return hdr, int(len(m1)), int(len(m7))\n\n    def _stock_text(ticker: str, for_date: str, news_df: pd.DataFrame, px: Dict[str, pd.DataFrame], k1: int = 80, k7: int = 160):\n        base = f\"CTX:STOCK | TICKER:{ticker} | DATE:{for_date}\\n\" + _stock_indicators(ticker, for_date, px)\n        if news_df is None or news_df.empty:\n            return base + \"NEWS_1D:\\nNone\\nNEWS_7D_EXCLUDING_1D:\\nNone\", 0, 0\n        w1, w7_ex = _window_time(news_df, for_date, NEWS_1D_HOURS, NEWS_7D_DAYS)\n        ref_dt = pd.to_datetime(for_date) + pd.Timedelta(hours=23, minutes=59)\n        s1 = _select_diverse_by_source(w1, k=k1, ref_dt=ref_dt, ticker=ticker)\n        s7 = _select_diverse_by_source(w7_ex, k=k7, ref_dt=ref_dt, ticker=ticker)\n        out = base + \"NEWS_1D:\\n\" + _format_lines(s1[\"headline\"].tolist())\n        out += \"\\nNEWS_7D_EXCLUDING_1D:\\n\" + _format_lines(s7[\"headline\"].tolist())\n        return out, int(len(s1)), int(len(s7))\n\n    user_text, _, _ = _market_text(date_eff, mkt_agg, px)\n    item_records = []\n    for t in tickers[:max_candidates]:\n        df_t = news_by_ticker.get(t, pd.DataFrame(columns=[\"datetime\",\"headline\",\"norm\",\"source\"]))\n        itxt, _, _ = _stock_text(t, date_eff, df_t, px)\n        item_records.append({\"item_id\": f\"I_{t}\", \"item_text\": itxt})\n\n    payload = {\n        \"date_eff\": date_eff,\n        \"user_record\": [{\"user_id\": f\"U_{date_eff}\", \"user_text\": user_text}],\n        \"item_records\": item_records,\n    }\n    return payload\n\n# ============================================================\n#               Remote: GPU inferencer (score)\n# ============================================================\n@app.function(\n    image=gpu_image,\n    gpu=[\"L4\", \"A10\", \"any\"],\n    volumes={\"/vol/models\": MODEL_VOL, \"/vol/code\": CODE_VOL},\n    timeout=60*20,\n)\ndef run_infer_remote(\n    payload: Dict[str, Any],\n    ckpt_path: str,\n    top_k: int = 20,\n    invest_amt: float = 1000.0,\n    device_choice: str = \"auto\",\n    temp: float = 2.0,\n) -> Dict[str, Any]:\n    def _ensure_interfusion_installed():\n        try:\n            import interfusion  # noqa: F401\n            return\n        except Exception:\n            import sys, subprocess, os\n            wheel_path = \"/vol/code/pkgs/interfusion_encoder-3.1.whl\"\n            src_path   = \"/vol/code/pkgs/interfusion_encoder-3.1\"\n            target = wheel_path if os.path.exists(wheel_path) else src_path\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", target])\n    _ensure_interfusion_installed()\n\n    import torch\n    import torch.nn as nn\n    import numpy as np\n\n    from interfusion.models import NewsTextEncoder, BiEncoderWithFeatures\n    from interfusion.trainer import (\n        Standardizer,\n        build_parsed_maps, build_cache,\n        encode_news_pool_trainable,\n        get_tqdm,\n        _topk_softmax_np,\n    )\n\n    def _torch_load_weights_only_false(path: str):\n        import inspect\n        kw = {\"map_location\": \"cpu\"}\n        if \"weights_only\" in inspect.signature(torch.load).parameters:\n            kw[\"weights_only\"] = False\n        return torch.load(path, **kw)\n\n    user_record = payload[\"user_record\"]\n    item_records = payload[\"item_records\"]\n    date_eff = payload[\"date_eff\"]\n\n    device = \"cuda\" if (device_choice == \"auto\" and torch.cuda.is_available()) else (device_choice if device_choice != \"auto\" else \"cpu\")\n\n    ckpt = _torch_load_weights_only_false(ckpt_path)\n    cfg = dict(ckpt.get(\"config\", {}))\n    cfg[\"device\"] = device\n    cfg.setdefault(\"online_encode_user_eval\", bool(cfg.get(\"online_encode_user\", True)))\n    cfg.setdefault(\"online_encode_items_eval\", bool(cfg.get(\"online_encode_items\", False)))\n    cfg[\"news_batch_size\"] = int(cfg.get(\"news_batch_size\", 128))\n\n    enc_name    = cfg.get(\"headline_encoder_name\", \"nreimers/BERT-Tiny_L-2_H-128_A-2\")\n    enc_max_len = int(cfg.get(\"headline_max_length\", 64))\n    freeze_enc  = bool(cfg.get(\"freeze_headline_encoder\", False))\n\n    news_enc = NewsTextEncoder(enc_name, max_length=enc_max_len, trainable=not freeze_enc).to(device)\n    if \"news_encoder_state_dict\" in ckpt:\n        _missing, _unexpected = news_enc.load_state_dict(ckpt[\"news_encoder_state_dict\"], strict=False)\n\n    news_dim = int(news_enc.output_dim)\n    model = BiEncoderWithFeatures(\n        user_num_dim=3,\n        item_num_dim=5,\n        news_emb_dim=news_dim,\n        proj_dim=int(cfg.get(\"proj_dim\", 128)),\n        d_model=int(cfg.get(\"d_model\", 256)),\n        normalize=bool(cfg.get(\"normalize_embeddings\", True)),\n    ).to(device)\n    model.load_state_dict(ckpt[\"model_state_dict\"], strict=True)\n    model.eval(); news_enc.eval()\n\n    u_std = Standardizer(); i_std = Standardizer()\n    nm = ckpt.get(\"numeric_stats\", {})\n    if nm:\n        if nm.get(\"user_mean\") is not None: u_std.mean = np.asarray(nm[\"user_mean\"], dtype=np.float32)\n        if nm.get(\"user_std\")  is not None: u_std.std  = np.asarray(nm[\"user_std\"],  dtype=np.float32)\n        if nm.get(\"item_mean\") is not None: i_std.mean = np.asarray(nm[\"item_mean\"], dtype=np.float32)\n        if nm.get(\"item_std\")  is not None: i_std.std  = np.asarray(nm[\"item_std\"],  dtype=np.float32)\n\n    tqdmm = get_tqdm(cfg)\n    user_parsed, item_parsed = build_parsed_maps(user_record, item_records, u_std, i_std, tqdmm)\n\n    online_user  = bool(cfg.get(\"online_encode_user_eval\", False))\n    online_items = bool(cfg.get(\"online_encode_items_eval\", False))\n    user_cache = {} if online_user  else build_cache(user_parsed, news_enc, int(cfg.get(\"news_batch_size\",128)), tqdmm)\n    item_cache = {} if online_items else build_cache(item_parsed, news_enc, int(cfg.get(\"news_batch_size\",128)), tqdmm)\n\n    uid = user_record[0][\"user_id\"]\n    pe_u = user_parsed[uid]\n    u_num = torch.from_numpy(pe_u.numeric).to(device).float().unsqueeze(0)\n\n    if online_user:\n        u_n1 = encode_news_pool_trainable(news_enc, pe_u.news1d_text)\n        u_n7 = encode_news_pool_trainable(news_enc, pe_u.news7d_text)\n    else:\n        ce = user_cache[uid]\n        u_n1 = torch.from_numpy(ce.news1d).to(device).float().unsqueeze(0)\n        u_n7 = torch.from_numpy(ce.news7d).to(device).float().unsqueeze(0)\n\n    ordered_ids = [r[\"item_id\"] for r in item_records]\n    it_nums, it_n1s, it_n7s = [], [], []\n    for iid in ordered_ids:\n        pe_i = item_parsed[iid]\n        it_nums.append(pe_i.numeric)\n        if online_items:\n            it_n1s.append(encode_news_pool_trainable(news_enc, pe_i.news1d_text))\n            it_n7s.append(encode_news_pool_trainable(news_enc, pe_i.news7d_text))\n        else:\n            ce = item_cache[iid]\n            it_n1s.append(torch.from_numpy(ce.news1d).to(device).float().unsqueeze(0))\n            it_n7s.append(torch.from_numpy(ce.news7d).to(device).float().unsqueeze(0))\n\n    it_num_t = torch.from_numpy(np.stack(it_nums)).to(device).float()\n    it_n1_t  = torch.cat(it_n1s, dim=0)\n    it_n7_t  = torch.cat(it_n7s, dim=0)\n\n    with torch.no_grad():\n        if isinstance(model, nn.DataParallel):\n            u_rep = model.module.encode_user(u_num.repeat(it_num_t.size(0),1), u_n1.repeat(it_num_t.size(0),1), u_n7.repeat(it_num_t.size(0),1))\n            v_rep = model.module.encode_item(it_num_t, it_n1_t, it_n7_t)\n        else:\n            u_rep = model.encode_user(u_num.repeat(it_num_t.size(0),1), u_n1.repeat(it_num_t.size(0),1), u_n7.repeat(it_num_t.size(0),1))\n            v_rep = model.encode_item(it_num_t, it_n1_t, it_n7_t)\n\n        scores = (u_rep * v_rep).sum(dim=-1).detach().cpu().numpy()\n\n    k = min(top_k, len(ordered_ids))\n    weights = _topk_softmax_np(scores, k=k, temp=float(temp))\n    order = np.argsort(scores)[::-1][:k]\n\n    def _iid_to_ticker(iid: str) -> str:\n        return iid.split(\"_\", 1)[-1] if \"_\" in iid else iid\n\n    rows = []\n    total_w = 0.0\n    total_d = 0.0\n    for rank, idx in enumerate(order, start=1):\n        iid = ordered_ids[idx]\n        tk  = _iid_to_ticker(iid)\n        sc  = float(scores[idx])\n        wi  = float(weights[idx])\n        alloc = float(invest_amt) * wi\n        total_w += wi; total_d += alloc\n        rows.append({\n            \"rank\": rank,\n            \"ticker\": tk,\n            \"item_id\": iid,\n            \"score\": sc,\n            \"weight\": wi,\n            \"allocation\": alloc\n        })\n\n    return {\n        \"date\": date_eff,\n        \"k\": k,\n        \"temp\": float(temp),\n        \"invest_amt\": float(invest_amt),\n        \"rows\": rows,\n        \"sum_weights\": total_w,\n        \"sum_allocation\": total_d,\n    }\n\n# ============================================================\n#     Plan creation (+ buy prices & quantities) on CPU\n# ============================================================\n@app.function(\n    image=base_image,\n    volumes={\"/vol/plans\": PLANS_VOL},\n    timeout=60*25,\n)\ndef create_investment_plan(\n    ckpt: str = DEFAULT_CKPT,\n    invest_amt: float = DEFAULT_INVEST_AMT,\n    top_k: int = DEFAULT_TOP_K,\n    max_candidates: int = DEFAULT_MAX_CANDIDATES,\n    date: str = DEFAULT_OVERRIDE_DATE,\n    temp: float = DEFAULT_TEMP,\n) -> Dict[str, Any]:\n    \"\"\"Create a new plan, compute buy prices & quantities, and persist it.\"\"\"\n    import yfinance as yf\n\n    payload = build_payload_remote.remote(\n        for_date=date or None,\n        tickers=TICKERS_UNIVERSE,\n        max_candidates=int(max_candidates),\n    )\n    result = run_infer_remote.remote(\n        payload=payload,\n        ckpt_path=ckpt,\n        top_k=int(top_k),\n        invest_amt=float(invest_amt),\n        temp=float(temp),\n    )\n\n    # Collect tickers in this plan\n    tickers = [r[\"ticker\"] for r in result[\"rows\"]]\n\n    # Get best-effort \"current\" price at plan time:\n    # prefer 1m data for today; if empty, fall back to 1d\n    def _last_close_map(tickers: List[str]) -> Dict[str, Optional[float]]:\n        if not tickers:\n            return {}\n        try:\n            d1m = yf.download(tickers, period=\"1d\", interval=\"1m\", group_by=\"ticker\", progress=False, threads=True, auto_adjust=False)\n        except Exception:\n            d1m = None\n        if d1m is None or isinstance(d1m, pd.DataFrame) and d1m.empty:\n            try:\n                d1d = yf.download(tickers, period=\"1d\", interval=\"1d\", group_by=\"ticker\", progress=False, threads=True, auto_adjust=False)\n            except Exception:\n                d1d = None\n        else:\n            d1d = None\n\n        price_map: Dict[str, Optional[float]] = {t: None for t in tickers}\n\n        def _extract(df: pd.DataFrame) -> Dict[str, float]:\n            out = {}\n            if df is None or df.empty:\n                return out\n            if isinstance(df.columns, pd.MultiIndex):\n                for t in set(df.columns.get_level_values(0)):\n                    if not isinstance(t, str): \n                        continue\n                    sub = df[t]\n                    if \"Close\" in sub and not sub[\"Close\"].empty:\n                        out[t] = float(sub[\"Close\"].iloc[-1])\n            else:\n                if \"Close\" in df and not df[\"Close\"].empty:\n                    out[\"SINGLE\"] = float(df[\"Close\"].iloc[-1])\n            return out\n\n        m1 = _extract(d1m) if d1m is not None else {}\n        m2 = _extract(d1d) if d1d is not None else {}\n        for t in tickers:\n            if t in m1 and m1[t] is not None:\n                price_map[t] = float(m1[t])\n            elif t in m2 and m2[t] is not None:\n                price_map[t] = float(m2[t])\n            else:\n                price_map[t] = None\n        return price_map\n\n    price_map = _last_close_map(tickers)\n\n    rows_out = []\n    for row in result[\"rows\"]:\n        tk = row[\"ticker\"]\n        alloc = float(row[\"allocation\"])\n        buy_price = price_map.get(tk)\n        if buy_price is None or buy_price <= 0:\n            quantity = None\n        else:\n            quantity = alloc / buy_price\n        rows_out.append({\n            **row,\n            \"buy_price\": buy_price,\n            \"quantity\": quantity,\n        })\n\n    now_utc = pd.Timestamp.utcnow()\n    plan_id = f\"{result['date']}_{now_utc.strftime('%Y%m%dT%H%M%SZ')}\"\n    plan = {\n        \"plan_id\": plan_id,\n        \"created_at\": now_utc.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n        \"date\": result[\"date\"],\n        \"k\": result[\"k\"],\n        \"temp\": result[\"temp\"],\n        \"invest_amt\": result[\"invest_amt\"],\n        \"rows\": rows_out,\n        \"sum_weights\": result[\"sum_weights\"],\n        \"sum_allocation\": result[\"sum_allocation\"],\n    }\n    _save_plan(plan)\n    return plan\n\n# ============================================================\n#    Attach live prices & compute PnL for a stored plan\n# ============================================================\n@app.function(image=base_image, volumes={\"/vol/plans\": PLANS_VOL}, timeout=60*20)\ndef _attach_live_prices_and_pnl(plan: Dict[str, Any]) -> Dict[str, Any]:\n    import yfinance as yf\n\n    tickers = [r[\"ticker\"] for r in plan.get(\"rows\", [])]\n    if not tickers:\n        return {\"plan\": plan, \"rows\": [], \"total_cost\": 0.0, \"total_value\": 0.0, \"total_pnl\": 0.0, \"total_pnl_pct\": 0.0}\n\n    def _live_price_map(tickers: List[str]) -> Dict[str, Optional[float]]:\n        # Try 1m first, then fall back to 1d\n        try:\n            d1m = yf.download(tickers, period=\"1d\", interval=\"1m\", group_by=\"ticker\", progress=False, threads=True, auto_adjust=False)\n        except Exception:\n            d1m = None\n        if d1m is None or (isinstance(d1m, pd.DataFrame) and d1m.empty):\n            try:\n                d1d = yf.download(tickers, period=\"1d\", interval=\"1d\", group_by=\"ticker\", progress=False, threads=True, auto_adjust=False)\n            except Exception:\n                d1d = None\n        else:\n            d1d = None\n\n        price_map: Dict[str, Optional[float]] = {t: None for t in tickers}\n\n        def _extract(df: pd.DataFrame) -> Dict[str, float]:\n            out = {}\n            if df is None or df.empty:\n                return out\n            if isinstance(df.columns, pd.MultiIndex):\n                for t in set(df.columns.get_level_values(0)):\n                    if not isinstance(t, str): continue\n                    sub = df[t]\n                    if \"Close\" in sub and not sub[\"Close\"].empty:\n                        out[t] = float(sub[\"Close\"].iloc[-1])\n            else:\n                if \"Close\" in df and not df[\"Close\"].empty:\n                    out[\"SINGLE\"] = float(df[\"Close\"].iloc[-1])\n            return out\n\n        m1 = _extract(d1m) if d1m is not None else {}\n        m2 = _extract(d1d) if d1d is not None else {}\n        for t in tickers:\n            if t in m1 and m1[t] is not None:\n                price_map[t] = float(m1[t])\n            elif t in m2 and m2[t] is not None:\n                price_map[t] = float(m2[t])\n            else:\n                price_map[t] = None\n        return price_map\n\n    price_map = _live_price_map(tickers)\n\n    rows = []\n    total_cost = 0.0\n    total_value = 0.0\n\n    for r in plan.get(\"rows\", []):\n        tk = r[\"ticker\"]\n        buy_price = r.get(\"buy_price\")\n        qty = r.get(\"quantity\")\n        live_price = price_map.get(tk)\n\n        if buy_price is None or qty is None or live_price is None:\n            position_value = None\n            pnl = None\n            pnl_pct = None\n            cost = 0.0\n        else:\n            cost = float(buy_price) * float(qty)\n            position_value = float(live_price) * float(qty)\n            pnl = position_value - cost\n            pnl_pct = (position_value / cost - 1.0) * 100.0\n            total_cost += cost\n            total_value += position_value\n\n        rows.append({\n            **r,\n            \"live_price\": live_price,\n            \"position_value\": position_value,\n            \"pnl\": pnl,\n            \"pnl_pct\": pnl_pct,\n        })\n\n    total_pnl = total_value - total_cost\n    total_pnl_pct = (total_value / total_cost - 1.0) * 100.0 if total_cost > 0 else 0.0\n\n    return {\n        \"plan\": plan,\n        \"rows\": rows,\n        \"total_cost\": total_cost,\n        \"total_value\": total_value,\n        \"total_pnl\": total_pnl,\n        \"total_pnl_pct\": total_pnl_pct,\n    }\n\n# ============================================================\n#                    Web endpoints (HTTP)\n# ============================================================\n@app.function(image=base_image, volumes={\"/vol/plans\": PLANS_VOL})\n@modal.web_endpoint(method=\"GET\")\ndef list_plans() -> Dict[str, Any]:\n    metas = _list_plans_meta()\n    return {\"plans\": metas}\n\n@app.function(image=base_image, volumes={\"/vol/plans\": PLANS_VOL})\n@modal.web_endpoint(method=\"GET\")\ndef get_plan_with_live(id: Optional[str] = None) -> Dict[str, Any]:\n    \"\"\"Return chosen plan with live prices & PnL. If none exist, create one.\"\"\"\n    plan_id = id\n    if not plan_id:\n        pid = _latest_plan_id()\n        if not pid:\n            # Create first plan\n            plan = create_investment_plan.remote()\n            return _attach_live_prices_and_pnl.remote(plan)\n        plan_id = pid\n\n    plan = _load_plan(plan_id)\n    if plan is None:\n        # If missing, lazily create a plan now\n        plan = create_investment_plan.remote()\n    return _attach_live_prices_and_pnl.remote(plan)\n\n# ============================================================\n#        Daily schedule at 9am Australia/Melbourne\n# ============================================================\n@app.function(\n    schedule=modal.Cron(\"0 9 * * *\", timezone=\"Australia/Melbourne\"),\n    image=base_image,\n    volumes={\"/vol/plans\": PLANS_VOL},\n    timeout=60*30,\n)\ndef scheduled_daily_plan():\n    return create_investment_plan.remote()\n\n# ============================================================\n#               Local entrypoint (manual trigger)\n# ============================================================\n@app.local_entrypoint()\ndef main():\n    # Manual: create a plan and print its ID, and show the two web URLs\n    plan = create_investment_plan.remote()\n    metas = _list_plans_meta()\n    print(\"Created plan_id:\", plan[\"plan_id\"])\n    print(\"Total plans:\", len(metas))\n    print(\"List plans URL   : /list_plans\")\n    print(\"Get latest plan  : /get_plan_with_live\")\n    print(\"Get a specific   : /get_plan_with_live?id=<PLAN_ID>\")\n"

def maybe_deploy_modal_app(auto: bool = False) -> str:
    """
    Writes the embedded Modal app to /tmp and deploys it using the Modal CLI.
    Returns the temp file path used for deployment.
    """
    tmp = Path(tempfile.gettempdir()) / "modal_infer_inmemory_latest.py"
    tmp.write_text(MODAL_APP_SOURCE, encoding="utf-8")
    if auto:
        try:
            subprocess.run(["modal", "deploy", str(tmp)], check=True, capture_output=True, text=True)
        except Exception as e:
            st.warning(f"Modal deploy failed (continuing anyway): {e}")
    return str(tmp)

def _lookup() -> tuple[modal.Function, modal.Function]:
    list_plans_fn = modal.Function.lookup(APP_NAME, "list_plans")
    get_plan_fn   = modal.Function.lookup(APP_NAME, "get_plan_with_live")
    return list_plans_fn, get_plan_fn

@st.cache_data(ttl=60, show_spinner=False)
def _get_plans(list_plans_fn: modal.Function) -> List[Dict[str, Any]]:
    data = list_plans_fn.call()
    return data.get("plans", [])

@st.cache_data(ttl=30, show_spinner=False)
def _get_plan_with_live(get_plan_fn: modal.Function, plan_id: Optional[str]) -> Dict[str, Any]:
    if plan_id:
        return get_plan_fn.call(id=plan_id)
    return get_plan_fn.call()

# --------------------------- UI ---------------------------
st.set_page_config(page_title="InterFusion Investment Plans", layout="wide")
st.title("Investment Plans (Modal GPU + Streamlit UI)")

st.sidebar.header("Modal control")
auto_deploy_default = os.getenv("AUTO_DEPLOY", "0") == "1"
deploy_now = st.sidebar.checkbox("Auto-deploy embedded Modal app on start", value=auto_deploy_default)
if st.sidebar.button("Deploy / Update Modal app now"):
    path = maybe_deploy_modal_app(auto=True)
    st.sidebar.success(f"Deployed from: {path}")
else:
    # If AUTO_DEPLOY=1, deploy quietly
    if deploy_now:
        maybe_deploy_modal_app(auto=True)

# Lookup deployed functions
try:
    list_plans_fn, get_plan_fn = _lookup()
except Exception as e:
    st.error("Couldn't find deployed Modal app. Try deploying from the sidebar, "
             "and verify MODAL_TOKEN_ID / MODAL_TOKEN_SECRET.")
    st.exception(e)
    st.stop()

# Load plans; create one if none exist
plans = _get_plans(list_plans_fn)
if not plans:
    st.info("No plans yet — creating one now on Modal...")
    _ = _get_plan_with_live(get_plan_fn, None)
    _get_plans.clear()
    plans = _get_plans(list_plans_fn)

if not plans:
    st.error("Still no plans after creation. Check Modal logs.")
    st.stop()

# Dropdown (oldest -> latest, default latest)
labels = [p["label"] for p in plans]
ids = [p["plan_id"] for p in plans]
default_idx = len(labels) - 1
sel_label = st.selectbox("Select investment plan", labels, index=default_idx)
sel_id = ids[labels.index(sel_label)]

# Fetch selected plan with live prices & PnL
data = _get_plan_with_live(get_plan_fn, sel_id)
if "error" in data:
    st.error(f"Modal returned an error: {data['error']}")
    st.stop()

plan = data["plan"]
rows = data.get("rows", [])
total_cost = data.get("total_cost", 0.0)
total_value = data.get("total_value", 0.0)
total_pnl = data.get("total_pnl", 0.0)
total_pnl_pct = data.get("total_pnl_pct", 0.0)

st.subheader(f"Plan for date {plan['date']}")
c1, c2, c3, c4 = st.columns(4)
c1.metric("Invest amount", f"${plan['invest_amt']:,.2f}")
c2.metric("Top-k", int(plan["k"]))
c3.metric("Current total value", f"${total_value:,.2f}")
c4.metric("Total P&L", f"${total_pnl:,.2f}", f"{total_pnl_pct:.2f}%")

st.markdown("---")

if not rows:
    st.warning("No rows in this plan.")
else:
    df = pd.DataFrame(rows)
    def m(x): return "—" if pd.isna(x) else f"${x:,.2f}"
    def q(x): return "—" if pd.isna(x) else f"{x:.4f}"
    df_disp = df.copy()
    for col, fmt in [
        ("allocation", m),
        ("buy_price", m),
        ("live_price", m),
        ("position_value", m),
        ("pnl", m),
        ("quantity", q),
    ]:
        if col in df_disp:
            df_disp[col] = df_disp[col].map(fmt)
    if "pnl_pct" in df_disp:
        df_disp["pnl_pct"] = df_disp["pnl_pct"].map(lambda v: "—" if pd.isna(v) else f"{v:.2f}%")
    cols = ["rank","ticker","item_id","allocation","buy_price","quantity","live_price","position_value","pnl","pnl_pct","score","weight"]
    cols = [c for c in cols if c in df_disp.columns]
    st.dataframe(df_disp[cols], use_container_width=True, hide_index=True)

st.caption("Not financial advice. Plans created daily at 9am Australia/Melbourne by the Modal app.")